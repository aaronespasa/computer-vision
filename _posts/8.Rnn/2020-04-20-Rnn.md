---
layout: post
title: "Teoría"
category: rnn
author: aaron
---

{% assign imgUrl = "/assets/8.Rnn/" | prepend: site.baseurl%}

{%comment%}
{%endcomment%}

# RNN's (Recurrent Neural Networks)

Son un tipo de redes neuronales que nos permiten no sólo considerar el único input de entrada, sino inputs con los que había trabajado en el pasado. A estos se les conoce como **dependencias temporales**. Por ello, podemos decir que tienen "memoria". Esto será que será crítico en el análisis de datos secuenciales como el análisis de texto o e aplicaciones en tiempo real.

Las RNN se llaman así porque llevan a cabo la misma tarea para cada elemento en la secuencia del input, es decir, de forma recurrente.



<img src="{{ "rnn.png" | prepend: imgUrl }}" class="md_image"/>

<p style="text-align:center">Representación gráfica de una RNN</p>

###### Ejemplos

Si, por ejemplo, nos imaginamos el frame de un vídeo en el que un gato está saltando  igual para nosotros podría ser relativamente sencillo distinguir la acción que está realizando. En cambio, para una máquina no lo sería. Por lo que esta necesitaría visualizar los frames anteriores del vídeo y, acordándose de los frames anteriores, poder clasificar la acción del gato. Este constituye un papel que pueden jugar las RNN's en el computer vision.

<img src="{{ "cat_jumping.jpg" | prepend: imgUrl }}" class="md_image"/>

<p style="text-align:center">Imagen de un gato saltando</p>

<br/>

También podríamos imaginarnos una pelota como la situada en la imagen inferior. ¿Serías capaz de predecir hacia donde se dirige? Por las leyes de la física podríamos deducir que hacia abajo, sin embargo puede que esté en movimiento por una patada y se dirija hacia la derecha, hacia arriba, hacia la izquierda, etc. Por lo tanto, deberíamos ver en qué posición se encontraba anteriormente para poder sacar conclusiones. Así es como funcionan las RNN, manteniendo **dependencias temporales** para así poder extraer conclusiones.

<img src="{{ "blue_ball.jpg" | prepend: imgUrl }}" class="md_image"/>

<p style="text-align:center">Bola sobre un fondo azul</p>

Si quieres ver cómo una RNN puede continuar tus dibujos, échale un ojo al siguiente link: [Aplicación web de dibujo con una RNN](https://magenta.tensorflow.org/assets/sketch_rnn_demo/index.html)

Por otra parte, si quieres leer cómo clasificar vídeos y generar subtítulos o una leyenda para ellos, visita el siguiente enlace: [Paper sobre el papel de las  RNN en el entendimiento de vídeos](https://video.udacity-data.com/topher/2018/May/5af0e03b_video-classification/video-classification.pdf)

### Historia

Tras la primera ola de redes neuronales artificiales a mediados de los años 80, uno de los problemas de estas surgía al no poder capturar dependencias temporales.

El primer intento para tratar de añadir memoria a las redes fue la [**Time Delay Neural Network**(TDNN)](https://en.wikipedia.org/wiki/Time_delay_neural_network) en 1989. En ellas, los inputs de anteriores pasos eran introducidos en la red. Sin embargo, estaban limitadas por el límite de tiempo escogido.

A las siguientes, propuestas en 1990, se les dió el nombre de RNN's simples (también conocidas como [la red de Jorda y la red de Elman](https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1)).

Pero a principios de los años 90 se reconoció que estas redes sufrían del llamado [**vanishing gradient problem**](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), el cual alegaba que las contribuciones de información iban decayendo con respecto al tiempo ([En series geométricas](https://en.wikipedia.org/wiki/Geometric_series)). Así que obtener relaciones procedientes a más de 10 pasos era prácticamete imposible.

A mediados de 1990 surgieron las [**Long Short-Term Memory**(LSTM)](http://www.bioinf.jku.at/publications/older/2604.pdf). La idea de estas es que algunas señales, llamadas **call state variables**, pueden mantenerse fijadas utilizando computertas y ser o no reintroducidas en un tiempo futuro. De esta forma se podían representar intervalos de tiempo arbitrarios y capturar dependencias temporales.

Más adelante, algunas variaciones en las LSTM's como las **Gated Recurrent Networks** refinarán los conceptos hablados anteriormente.

### Aplicaciones

- **Reconocimiento de voz**: Secuencias de ejemplos de datos extraídos de una señal de audio son continuamente representados como texto. Ej. Google Assistant.
- **Time series prediction**:
  - Patrones de tráfico: Predicción de patrones de tráfico en rutas específicas para ayudar al conductor a optimizar sus trayectos de conducción. Ej. Waze.
  - Selección de películas: Mostrar una recomendación de la película que ver a continuación según nuestros gustos. Ej. Netflix.
  - Movimientos y condiciones de la bolsa y el mercado: Predecir los movimientos que se realizarán en la bolsa en base a patrones históricos u otros movimientos del mercado que se harán a lo largo del tiempo.
  - [Añadir automáticamente silencios a las películas](https://www.youtube.com/watch?time_continue=1&v=0FW99AQmMc8).
  - ...
- **Procesado de lenguaje natural (NLP)**:
  - Machine translation. Ej. Google translate.
  - Respuesta a preguntas. Ej. Google Analytics para hacer preguntas sobre nuestra app.
  - Chatbots. Ej. Chatbots de slack, discord, DotA 2 bot de Open AI, ...
  - ...
- **Reconocimiento de gestos**. Algunas compañías que lo están aplicando a sus productos son Intel, Qualcomm, EyeSight, ...

