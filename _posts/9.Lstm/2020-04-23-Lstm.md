---
layout: post
title: "Teoría"
category: lstm
author: aaron
---

{% assign imgUrl = "/assets/9.Lstm/" | prepend: site.baseurl%}

# Long Short Term Memory networks (LSTM)
<img src="{{ "lstm_diagram.gif" | prepend: imgUrl }}" class="md_image"/>

<p style="text-align:center">Representación de las neuronas de una LSTM - Imagen de Michael Nguyen</p>

Como he hablado en secciones anteriores, las RNN's poseían el **vanishing gradient problem**, recaptiulando, tras 10 estados de memoria el gradiente comenzaba a aproximarse a 0 poseyendo así una memoria a corto plazo. Si se le suministraba una secuencia lo suficientemente larga a una Simple RNN le costaría llevar la información hasta los últimos pasos. Para solucionar esto surgieron las <a href="https://www.bioinf.jku.at/publications/older/2604.pdf" target="_blank">**Long Short Term Memory networks**</a> de las cuales hablaré en este artículo.<br/>

Muchas de las ilustraciones y GIF que voy a utilizar pertenecen al artículo <a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21" target="_blank">"Una guía ilustrada de LSTM's y GRU's: Explicación paso a paso" - Michael Nguyen</a> el cual recomiendo echar un ojo.

<br/>

<img src="{{ "lstm_diagram.png" | prepend: imgUrl }}" class="md_image"/>

<p style="text-align:center">Neurona de una LSTM - Imagen de Udacity</p>

El principal componente de una LSTM son sus neuronas y creo que para entender cómo funciona esta red lo mejor es comenzar entendiéndolas.

### Memoria a corto y largo plazo (Long and Short memory)

Fijémonos en los inputs:

E<sub>t</sub> es el vector que recibe la neurona para computar.

LTM<sub>t-1</sub> representa la memoria del bloque anterior, esta es la **memoria a largo plazo**. Y STM<sub>t-1</sub>, al contrario que el anterior, constituye el output del bloque anterior, es decir, la **memoria a corto plazo**.

Y respecto a los outputs:

STM<sub>t</sub> es el output de la neurona y, a su vez, representa la **memoria a corto plazo** que será alimentada al siguiente bloque. Por otro lugar, LTM<sub>t</sub> es la **memoria a largo plazo** del bloque ya computada para ser transferida también al siguiente bloque.

Para ver la implementación en código: <a href="./Lstm-Inputs-Outputs" target="_blank">Inputs y Outputs de una LSTM en código</a>

### Compuertas (Gates)

La forma en la que la neurona computa los inputs para dar lugar a los outputs es mediante una serie de computertas:

<img src="{{ "lstm_gates_diagram.png" | prepend: imgUrl }}" class="md_image"/>

<p style="text-align:center">Compuertas de una LSTM - Imagen modificada de Michael</p>

#### Input Gate (or Learn Gate)

<img src="{{ "input_gate.gif" | prepend: imgUrl }}" class="md_image"/>

<p style="text-align:center">Input Gate - Imagen de Michael</p>

Toma el input vector y la short term memory y los une. Entonces ignora ciertas partes para conservar únicamente los datos importantes de él.

Para combinar el input vector con la short term memory:

1. Se forma una matriz con el input vector y la short term memory.
2. Este se multiplica por la matriz de ponderaciones.
3. Se añade el término de bias.
4. Se aplica la función de activación tanh

$$
N_t = tanh(W_n * [STM_{t-1}, E_t] + b_n)
$$

Para ignorar parte de él:

Se multiplica por un ignore factor (i<sub>t</sub>), es decir, un vector que mulplica cada elemento. Para calcularlo:

Se repite el procedimiento anterior, sin embargo, se utiliza la función de activación sigmoide (para mantener los valores entre 0 y 1) y tanto un nuevo vector de ponderaciones como un nuevo término de bias.


$$
i_t = \sigma(W_i * [STM_{t-1}, E_t] + b_i)
$$


<br/>

<img src="{{ "learn_gate_mathematically.png" | prepend: imgUrl }}" class="md_image"/>

<p style="text-align:center">Representación matemática de la input gate - Imagen de Udacity</p>