{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiendo una LSTM simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      " [tensor([[-0.7739,  0.0496, -0.6174,  2.2406]]), tensor([[ 1.3194, -1.3208, -0.2356, -0.3233]]), tensor([[0.0477, 1.0690, 1.0751, 0.3943]]), tensor([[ 1.1772, -0.7236, -0.2669,  0.2690]]), tensor([[-0.5728,  2.2614, -0.5147,  0.5446]])]\n",
      "\n",
      "out: \n",
      " tensor([[[-0.4678,  0.0041, -0.2471]]], grad_fn=<StackBackward>)\n",
      "hidden: \n",
      " (tensor([[[-0.4678,  0.0041, -0.2471]]], grad_fn=<StackBackward>), tensor([[[-0.6262,  0.0332, -0.3358]]], grad_fn=<StackBackward>))\n",
      "\n",
      "out: \n",
      " tensor([[[-0.3292, -0.2225,  0.4648]]], grad_fn=<StackBackward>)\n",
      "hidden: \n",
      " (tensor([[[-0.3292, -0.2225,  0.4648]]], grad_fn=<StackBackward>), tensor([[[-0.4465, -0.5758,  0.6697]]], grad_fn=<StackBackward>))\n",
      "\n",
      "out: \n",
      " tensor([[[-0.0103, -0.0617,  0.4570]]], grad_fn=<StackBackward>)\n",
      "hidden: \n",
      " (tensor([[[-0.0103, -0.0617,  0.4570]]], grad_fn=<StackBackward>), tensor([[[-0.0241, -0.4090,  0.6000]]], grad_fn=<StackBackward>))\n",
      "\n",
      "out: \n",
      " tensor([[[-0.3421, -0.1574,  0.4006]]], grad_fn=<StackBackward>)\n",
      "hidden: \n",
      " (tensor([[[-0.3421, -0.1574,  0.4006]]], grad_fn=<StackBackward>), tensor([[[-0.4899, -0.5758,  0.5336]]], grad_fn=<StackBackward>))\n",
      "\n",
      "out: \n",
      " tensor([[[ 0.0059, -0.0539,  0.0390]]], grad_fn=<StackBackward>)\n",
      "hidden: \n",
      " (tensor([[[ 0.0059, -0.0539,  0.0390]]], grad_fn=<StackBackward>), tensor([[[ 0.0109, -0.5090,  0.0439]]], grad_fn=<StackBackward>))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_dim = 4\n",
    "hidden_dim = 3\n",
    "# nn.LSTM(input_size, hidden_size, num_layers)\n",
    "# :param input_size:  Number of inputs\n",
    "# :param hidden_size: Size of the hidden state,\n",
    "#                     a function which contains the weights and represent the long and short term memory component.\n",
    "#                     This will be the number of outputs that each LSTM cell produces at each time step.\n",
    "# :param num_layers:  Number of hidden LSTM layers. Default value: 1.\n",
    "lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim)\n",
    "\n",
    "# Make 5 input sentences of 4 random values each\n",
    "inputs_list = [torch.randn(1, input_dim) for _ in range(5)]\n",
    "print('inputs: \\n', inputs_list, end=\"\\n\\n\")\n",
    "\n",
    "# initialize the hidden state\n",
    "# (1 layer, 1 batch_size, 3 outputs)\n",
    "# first tensor is the hidden state, h0\n",
    "# second tensor initializes the cell memory, c0\n",
    "h0 = torch.randn(1, 1, hidden_dim)\n",
    "c0 = torch.randn(1, 1, hidden_dim)\n",
    "\n",
    "# step through the sequence one element at a time.\n",
    "for i in inputs_list:\n",
    "    # after each step, hidden contains the hidden state\n",
    "    out, hidden = lstm(i.view(1, 1, -1), (h0, c0))\n",
    "    print('out: \\n', out)\n",
    "    print('hidden: \\n', hidden, end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesar todos los inputs a la misma vez\n",
    "Procedimiento:\n",
    "1. Concatenar todas nuestras secuencias de inputs e un único gran tensor, con un batch_size definido\n",
    "2. Definir el tamaño de nuestro hidden state\n",
    "3. Obtener los outputs y los hidden state más recientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs size: \n",
      " torch.Size([5, 1, 4])\n",
      "\n",
      "inputs: \n",
      " tensor([[[-0.7739,  0.0496, -0.6174,  2.2406]],\n",
      "\n",
      "        [[ 1.3194, -1.3208, -0.2356, -0.3233]],\n",
      "\n",
      "        [[ 0.0477,  1.0690,  1.0751,  0.3943]],\n",
      "\n",
      "        [[ 1.1772, -0.7236, -0.2669,  0.2690]],\n",
      "\n",
      "        [[-0.5728,  2.2614, -0.5147,  0.5446]]])\n",
      "\n",
      "out: \n",
      " tensor([[[-0.3707,  0.0372, -0.0486]],\n",
      "\n",
      "        [[-0.2980, -0.1198,  0.2102]],\n",
      "\n",
      "        [[-0.0199, -0.0067,  0.4572]],\n",
      "\n",
      "        [[-0.0158, -0.0862,  0.4438]],\n",
      "\n",
      "        [[ 0.0961, -0.0342,  0.2408]]], grad_fn=<StackBackward>)\n",
      "\n",
      "hidden: \n",
      " (tensor([[[ 0.0961, -0.0342,  0.2408]]], grad_fn=<StackBackward>), tensor([[[ 0.4994, -0.2241,  0.3038]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "# turn inputs into a tensor with 5 rows of data\n",
    "# add the extra 2nd dimension (1) for batch_size\n",
    "inputs = torch.cat(inputs_list).view(len(inputs_list), 1, -1)\n",
    "\n",
    "# print out our inputs and their shape\n",
    "# you should see (number of sequences, batch size, input_dim)\n",
    "print('inputs size: \\n', inputs.size(), end=\"\\n\\n\")\n",
    "print('inputs: \\n', inputs, end=\"\\n\\n\")\n",
    "\n",
    "# initialize the hidden state\n",
    "h0 = torch.randn(1, 1, hidden_dim)\n",
    "c0 = torch.randn(1, 1, hidden_dim)\n",
    "\n",
    "# get the outputs and hidden state\n",
    "out, hidden = lstm(inputs, (h0, c0))\n",
    "\n",
    "print('out: \\n', out, end=\"\\n\\n\")\n",
    "print('hidden: \\n', hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
